{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from genai_evaluator.clients import OpenAILLMClient, TemplateStore\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = OpenAILLMClient(\n",
    "    key=AZURE_OPENAI_API_KEY,\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    model=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading templates from genai_evaluator\\templates\n"
     ]
    }
   ],
   "source": [
    "from genai_evaluator.clients.data_clients import TemplateStore\n",
    "\n",
    "template_store = TemplateStore(dir_path=\"genai_evaluator/templates\", do_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevancy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from genai_evaluator.flows.retrieval_eval_flow import retrieval_eval_flow\n",
    "\n",
    "dict_eval_ret = retrieval_eval_flow(\n",
    "    question=\"What is the capital of France?\",\n",
    "    answer_gt=\"The capital of France is Paris.\",\n",
    "    answer_pred=\"France's capital city is Paris, known for the Eiffel Tower.\",\n",
    "    context=[\n",
    "        \"Paris is the capital and most populous city of France.\",\n",
    "        \"The Eiffel Tower, located in Paris, is a famous landmark.\",\n",
    "        \"France is a country in Western Europe known for its art, culture, and cuisine.\",\n",
    "        \"Lyon and Marseille are also major French cities but not the capital.\",\n",
    "    ],\n",
    "    llm_client=llm_client,\n",
    "    template_store=template_store,\n",
    ")\n",
    "\n",
    "print(dict_eval_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt flow service...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-22 09:08:34 +0200][promptflow][WARNING] - The starting prompt flow process did not finish within the timeout period. Kindly reminder: If you have previously upgraded the prompt flow package , please double-confirm that you have run '\u001b[1mpf service stop\u001b[0m' to stop the prompt flowservice before proceeding with the upgrade. Otherwise, you may encounter unexpected environmental issues or inconsistencies between the version of running prompt flow service and the local prompt flow version. Alternatively, you can use the '\u001b[1mpf upgrade\u001b[0m' command to proceed with the upgrade process for the prompt flow package.\n",
      "[2025-05-22 09:08:36 +0200][promptflow][WARNING] - Prompt flow service is not healthy. Kindly reminder: If you have previously upgraded the prompt flow package , please double-confirm that you have run '\u001b[1mpf service stop\u001b[0m' to stop the prompt flowservice before proceeding with the upgrade. Otherwise, you may encounter unexpected environmental issues or inconsistencies between the version of running prompt flow service and the local prompt flow version. Alternatively, you can use the '\u001b[1mpf upgrade\u001b[0m' command to proceed with the upgrade process for the prompt flow package.\n",
      "[2025-05-22 09:08:38 +0200][promptflow][WARNING] - Prompt flow service is not healthy, please check the logs for more details; traces might not be exported correctly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 1.0, 'precision': 1.0, 'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from promptflow.tracing import start_trace\n",
    "\n",
    "from genai_evaluator.flows.generation_eval_flow import generation_eval_flow\n",
    "\n",
    "os.environ[\"PF_DISABLE_TRACING\"] = \"FALSE\"\n",
    "start_trace(collection=\"trace_rag_metrics\")\n",
    "\n",
    "dict_eval_gen = generation_eval_flow(\n",
    "    question=\"What is the capital of France?\",\n",
    "    answer_gt=\"The capital of France is Paris.\",\n",
    "    answer_pred=\"France's capital city is Paris, known for the Eiffel Tower.\",\n",
    "    context=[\n",
    "        \"Paris is the capital and most populous city of France.\",\n",
    "        \"The Eiffel Tower, located in Paris, is a famous landmark.\",\n",
    "        \"France is a country in Western Europe known for its art, culture, and cuisine.\",\n",
    "        \"Lyon and Marseille are also major French cities but not the capital.\",\n",
    "    ],\n",
    "    llm_client=llm_client,\n",
    "    template_store=template_store,\n",
    ")\n",
    "\n",
    "print(dict_eval_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from genai_evaluator.flows.retrieval_eval_flow import retrieval_eval_flow\n",
    "\n",
    "dict_eval_ret = retrieval_eval_flow(\n",
    "    question=\"What are the main ingredients in traditional Italian pesto?\",\n",
    "    answer_gt=\"Traditional Italian pesto is made with basil, garlic, pine nuts, Parmesan cheese, and olive oil.\",\n",
    "    answer_pred=\"Italian pesto is usually made with spinach, almonds, cheddar cheese, and olive oil.\",\n",
    "    context=[\n",
    "        \"Pesto alla Genovese, the traditional Italian pesto, is made with fresh basil leaves, garlic, pine nuts, Parmesan cheese, and extra virgin olive oil.\",\n",
    "        \"Spinach and other ingredients may be used in variations of pesto, but they are not part of the traditional recipe.\",\n",
    "        \"The use of almonds or cheddar cheese is not typical in authentic Italian pesto recipes.\",\n",
    "        \"Sunflower oil is not used in traditional Italian pesto.\",\n",
    "    ],\n",
    "    llm_client=llm_client,\n",
    "    template_store=template_store,\n",
    ")\n",
    "print(dict_eval_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "{'faithfulness': 0.25, 'precision': 1.0, 'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from promptflow.tracing import start_trace\n",
    "\n",
    "os.environ[\"PF_DISABLE_TRACING\"] = \"FALSE\"\n",
    "\n",
    "\n",
    "start_trace()\n",
    "dict_eval_gen = generation_eval_flow(\n",
    "    question=\"What are the main ingredients in traditional Italian pesto?\",\n",
    "    answer_gt=\"Traditional Italian pesto is made with basil, garlic, pine nuts, Parmesan cheese, and olive oil.\",\n",
    "    answer_pred=\"Italian pesto is usually made with spinach, almonds, cheddar cheese, and olive oil.\",\n",
    "    context=[\n",
    "        \"Pesto alla Genovese, the traditional Italian pesto, is made with fresh basil leaves, garlic, pine nuts, Parmesan cheese, and extra virgin olive oil.\",\n",
    "        \"Spinach and other ingredients may be used in variations of pesto, but they are not part of the traditional recipe.\",\n",
    "        \"The use of almonds or cheddar cheese is not typical in authentic Italian pesto recipes.\",\n",
    "        \"Sunflower oil is not used in traditional Italian pesto.\",\n",
    "    ],\n",
    "    llm_client=llm_client,\n",
    "    template_store=template_store,\n",
    ")\n",
    "print(dict_eval_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG evaluation flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_evaluator.clients.embedding_client import OpenAIEmbeddingClient\n",
    "from genai_evaluator.clients.vector_store_client import FAISSClient\n",
    "from genai_evaluator.flows.rag_flow import rag_flow\n",
    "\n",
    "embedding_client = OpenAIEmbeddingClient(\n",
    "    credential=AZURE_OPENAI_API_KEY,\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "vector_store = FAISSClient(embedding_client=embedding_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-22 10:02:20.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenai_evaluator.clients.vector_store_client\u001b[0m:\u001b[36mingest_documents\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mTotal documents ingested: 17012\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "context = Path(\"../data/equality-and-diversity-policy.pdf\")\n",
    "batch_size =  400\n",
    "# context = [\n",
    "#     \"To build muscle effectively, focus on compound exercises such as squats, deadlifts, bench press, and pull-ups.\",\n",
    "#     \"Progressive overload is essential — gradually increasing the weight or reps over time will stimulate muscle growth.\",\n",
    "#     \"Eating a calorie surplus with adequate protein intake (1.6-2.2g per kg of body weight) supports muscle hypertrophy.\",\n",
    "#     \"Rest and recovery are crucial; muscles grow during rest periods, so aim for at least 7-8 hours of sleep per night.\",\n",
    "#     \"Train each major muscle group at least twice a week with a combination of volume and intensity.\",\n",
    "#     \"A typical beginner mass-building routine includes 3–4 full-body workouts per week focusing on strength and hypertrophy.\",\n",
    "#     \"Supplements like creatine monohydrate and whey protein can support your muscle-building goals when used correctly.\",\n",
    "# ]\n",
    "vector_store.ingest_documents(context, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the best exercises for building muscle?\"\n",
    "answer_gt = \"The best exercises for building muscle include compound movements like squats, deadlifts, bench press, and pull-ups.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=trace_rag_metrics&uiTraceId=0x34a19632cec1143b3c713012cd8e3ad7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type dict for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type dict for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type dict for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type dict for attribute 'llm.usage.prompt_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type dict for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevancy': 1.0, 'faithfulness': 0.0, 'precision': 0, 'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "\n",
    "os.environ[\"PF_DISABLE_TRACING\"] = \"FALSE\"\n",
    "start_trace(collection=\"trace_rag_metrics\")\n",
    "\n",
    "result = rag_flow(\n",
    "    question=question,\n",
    "    answer_gt=answer_gt,\n",
    "    llm_client=llm_client,\n",
    "    template_store=template_store,\n",
    "    vector_store=vector_store,\n",
    "    num_results=3,\n",
    "    system_prompt=\"Answer the question based on the provided context.\",\n",
    ")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
